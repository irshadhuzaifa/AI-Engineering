{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2009ea2",
   "metadata": {},
   "source": [
    "# Lecture 3: Understanding Embeddings and Representations in AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "AI models need to process and understand complex data like text, images, and audio. However, raw data cannot be used directly; it must first be converted into numerical representations that AI models can process efficiently.\n",
    "\n",
    "Embeddings are dense vector representations of data that capture semantic meaning and relationships between different entities. They are widely used in Natural Language Processing (NLP), computer vision, recommendation systems, and retrieval-based AI models.\n",
    "\n",
    "### In this lecture, we will explore:\n",
    "1. **What embeddings are and why they are important**\n",
    "2. **How embeddings are created and trained**\n",
    "3. **Real-world applications of embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What are Embeddings?\n",
    "\n",
    "### A. The Concept of Embeddings\n",
    "\n",
    "An embedding is a vector representation of an entity (such as a word, image, or user behavior) in a continuous space where similar entities are placed closer together.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> In a word embedding model, the words `king` and `queen` will have similar vector representations because they are semantically related.\n",
    "\n",
    "#### ðŸ”¹ Why Use Embeddings?\n",
    "- Convert high-dimensional data (words, images) into compact numerical representations.\n",
    "- Capture semantic meaning (e.g., synonyms have similar embeddings).\n",
    "- Enable AI models to generalize and understand context better.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How are Embeddings Created?\n",
    "\n",
    "### A. Training Embeddings from Data\n",
    "\n",
    "Embeddings are typically learned using deep learning models that analyze relationships between data points.\n",
    "\n",
    "#### ðŸ”¹ Methods for Learning Embeddings:\n",
    "- âœ… **Word2Vec** â†’ Predicts word relationships based on context.\n",
    "- âœ… **GloVe (Global Vectors for Word Representation)** â†’ Captures word co-occurrences.\n",
    "- âœ… **BERT Embeddings** â†’ Contextual word embeddings for NLP.\n",
    "- âœ… **CLIP Embeddings** â†’ Maps text and images into the same vector space.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Applications of Embeddings in AI\n",
    "\n",
    "### A. Natural Language Processing (NLP)\n",
    "\n",
    "- ðŸ”¹ **Text Similarity:** Finding similar sentences using cosine similarity between word embeddings.\n",
    "- ðŸ”¹ **Machine Translation:** Models like BERT use embeddings to understand context in different languages.\n",
    "\n",
    "**ðŸ“Œ Example:** Google Translate uses embeddings to represent words across languages.\n",
    "\n",
    "### B. Computer Vision\n",
    "\n",
    "- ðŸ”¹ **Image Embeddings:** CNNs generate vector representations of images for classification.\n",
    "- ðŸ”¹ **Cross-Modal Search:** CLIP embeddings allow AI to understand relationships between text and images.\n",
    "\n",
    "**ðŸ“Œ Example:** CLIP by OpenAI enables AI to search for images using text descriptions.\n",
    "\n",
    "### C. Recommendation Systems\n",
    "\n",
    "- ðŸ”¹ **User Embeddings:** Online platforms (Netflix, Spotify) create embeddings for users based on their interactions.\n",
    "- ðŸ”¹ **Product Recommendations:** Amazon uses embeddings to suggest related items.\n",
    "\n",
    "**ðŸ“Œ Example:** Spotifyâ€™s AI models use song embeddings to recommend personalized playlists.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Case Study: AI-Powered Search Engine\n",
    "\n",
    "- ðŸ”¹ **Problem:** Traditional keyword-based search fails to understand meaning.\n",
    "- ðŸ”¹ **Solution:** AI-powered search engine using vector embeddings.\n",
    "- ðŸ”¹ **Workflow:**\n",
    "  - âœ… **Text Embeddings:** Convert queries and documents into vectors.\n",
    "  - âœ… **Vector Search:** Use a vector database (FAISS, Pinecone) to retrieve relevant results.\n",
    "  - âœ… **Ranking Model:** Score and display the most relevant documents.\n",
    "\n",
    "ðŸ’¡ **Result:** The AI search engine delivers more context-aware and relevant results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb646ed",
   "metadata": {},
   "source": [
    "### Word Embeddings with Word2Vec\n",
    "\n",
    "#### ðŸ“Œ Training word embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99529286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Embedding: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
      "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"AI\"],\n",
    "    [\"AI\", \"is\", \"amazing\"],\n",
    "    [\"Embeddings\", \"capture\", \"meaning\"]\n",
    "]\n",
    "\n",
    "# Function to train Word2Vec model\n",
    "def train_word2vec(sentences, vector_size=10, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = train_word2vec(sentences)\n",
    "\n",
    "# Function to get the embedding of a word\n",
    "def get_word_embedding(model, word):\n",
    "    if word in model.wv:\n",
    "        return model.wv[word]\n",
    "    else:\n",
    "        raise ValueError(f\"The word '{word}' is not in the vocabulary.\")\n",
    "\n",
    "# Get vector representation of \"AI\"\n",
    "try:\n",
    "    ai_embedding = get_word_embedding(model, \"AI\")\n",
    "    print(\"AI Embedding:\", ai_embedding)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eac833",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "### Importing Word2Vec  \n",
    "The `gensim.models.Word2Vec` module is used to train word embeddings, which represent words as numerical vectors.\n",
    "\n",
    "### Creating a Sample Dataset  \n",
    "A small list of sentences is defined, where each sentence is a list of words (tokens).\n",
    "\n",
    "### Training Function  \n",
    "The `train_word2vec` function trains a Word2Vec model using the following parameters:  \n",
    "\n",
    "- **vector_size=10**: Each word is represented by a 10-dimensional vector.  \n",
    "- **window=5**: Considers 5 words before and after the target word as context.  \n",
    "- **min_count=1**: Includes words appearing at least once.  \n",
    "- **workers=4**: Uses 4 CPU threads for faster processing.  \n",
    "\n",
    "### Training the Model  \n",
    "The function is called with the dataset to create a trained model.\n",
    "\n",
    "### Fetching Word Embeddings  \n",
    "The `get_word_embedding` function retrieves the vector representation of a word if it exists in the model's vocabulary. If the word is missing, an error is raised.\n",
    "\n",
    "### Retrieving \"AI\" Embedding  \n",
    "The script attempts to fetch the embedding for `\"AI\"`. If successful, it prints the vector; otherwise, it displays an error message.\n",
    "\n",
    "---\n",
    "\n",
    "This code essentially trains a simple **Word2Vec** model and demonstrates how to extract meaningful vector representations of words. ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9798c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
