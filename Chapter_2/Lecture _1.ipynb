{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6366cb",
   "metadata": {},
   "source": [
    "# Lecture 1: What is a Neural Network? Basics of Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Neural networks are the building blocks of modern AI and power applications like self-driving cars, chatbots, and image recognition. Unlike traditional programs, which follow predefined rules, neural networks learn from data by recognizing patterns.\n",
    "\n",
    "### In this lecture, we will cover:\n",
    "1. What neural networks are and how they work\n",
    "2. Different types of neural networks\n",
    "3. How neural networks learn through training\n",
    "\n",
    "By the end, you‚Äôll have a clear understanding of how neural networks function and why they are essential in AI.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding Neural Networks\n",
    "\n",
    "### A. What is a Neural Network?\n",
    "\n",
    "A neural network is a system of artificial neurons that work together to process information. It is inspired by how the human brain works.\n",
    "\n",
    "Imagine a self-driving car trying to recognize a stop sign üö¶:\n",
    "\n",
    "- The car‚Äôs camera captures an image.\n",
    "- The neural network analyzes the image and detects patterns.\n",
    "- It determines whether the image contains a stop sign.\n",
    "\n",
    "### B. Structure of a Neural Network\n",
    "\n",
    "A neural network is made up of three layers:\n",
    "\n",
    "| Layer         | Function                                      |\n",
    "|--------------|----------------------------------------------|\n",
    "| Input Layer  | Receives raw data (e.g., images, text, numbers). |\n",
    "| Hidden Layers | Process data and detect patterns.           |\n",
    "| Output Layer | Makes the final prediction (e.g., stop sign or no stop sign). |\n",
    "\n",
    "üîπ **Example: Recognizing handwritten digits (0-9) using a neural network.**\n",
    "\n",
    "- **Input**: The pixels of the image (grayscale values).\n",
    "- **Hidden Layers**: Extract important features (like curves or lines).\n",
    "- **Output**: The number the image represents.\n",
    "\n",
    "üìå **Think of a neural network like a chef preparing a meal:**\n",
    "\n",
    "- **Ingredients (input layer)** ‚Üí Raw data.\n",
    "- **Cooking process (hidden layers)** ‚Üí Transforming data into something useful.\n",
    "- **Final dish (output layer)** ‚Üí The result (e.g., prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Types of Neural Networks\n",
    "\n",
    "Neural networks come in different types, depending on what they are used for.\n",
    "\n",
    "### A. Multi-Layer Perceptron (MLP) ‚Äì The Basic Neural Network\n",
    "\n",
    "- The simplest type of neural network.\n",
    "- Used for tasks like spam detection (email is spam or not).\n",
    "\n",
    "### B. Convolutional Neural Networks (CNNs) ‚Äì For Images\n",
    "\n",
    "- Specially designed for image recognition.\n",
    "- Used in self-driving cars, facial recognition, and medical imaging.\n",
    "\n",
    "### C. Recurrent Neural Networks (RNNs) ‚Äì For Sequences\n",
    "\n",
    "- Processes time-based data like speech, music, and stock prices.\n",
    "- Used in chatbots and real-time translation (Google Translate).\n",
    "\n",
    "üìå **Think of different neural networks like different types of athletes:**\n",
    "\n",
    "- **MLP** ‚Üí A runner (simple tasks).\n",
    "- **CNN** ‚Üí A gymnast (identifies shapes and patterns).\n",
    "- **RNN** ‚Üí A musician (remembers past data and sequences).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How Neural Networks Learn (Training Process)\n",
    "\n",
    "### A. The Learning Process\n",
    "\n",
    "Neural networks learn by adjusting their internal connections (weights and biases).\n",
    "\n",
    "üîπ **Example: Teaching a child to recognize apples üçè**\n",
    "\n",
    "1. Show the child pictures of apples and non-apples.\n",
    "2. If they make a mistake, correct them.\n",
    "3. Repeat the process until they can recognize apples correctly.\n",
    "\n",
    "Neural networks learn in a similar way by:\n",
    "\n",
    "1. Receiving input data (e.g., an image of an apple).\n",
    "2. Making a prediction (e.g., \"this is an apple\").\n",
    "3. Checking the error (Was the prediction correct?).\n",
    "4. Adjusting itself to make better future predictions.\n",
    "\n",
    "### B. The Role of Activation Functions\n",
    "\n",
    "Activation functions decide whether a neuron should \"fire\" (activate) based on its input.\n",
    "\n",
    "| Activation Function | Use Case                                  |\n",
    "|--------------------|-----------------------------------------|\n",
    "| Sigmoid           | Used in binary classification (yes/no problems). |\n",
    "| ReLU              | Most common, used in deep networks. |\n",
    "| Softmax           | Used for multi-class classification. |\n",
    "\n",
    "üìå **Think of activation functions like a decision filter:**\n",
    "\n",
    "- **Sigmoid**: Decides if a light switch should turn on/off.\n",
    "- **ReLU**: Ignores weak signals but reacts to strong ones.\n",
    "\n",
    "### C. How Training Works: The Role of Errors\n",
    "\n",
    "- After each prediction, the network checks how wrong or right it was.\n",
    "- It adjusts itself using an optimization process called **backpropagation**.\n",
    "- Over time, the network improves, just like a student who learns from mistakes.\n",
    "\n",
    "üìå **Think of backpropagation like a coach helping an athlete:**\n",
    "\n",
    "- The coach points out mistakes (**error feedback**).\n",
    "- The athlete adjusts their technique (**learning**).\n",
    "- Over time, performance improves.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Case Study: AI-Powered Handwriting Recognition\n",
    "\n",
    "üîπ **Problem**: Traditional OCR (Optical Character Recognition) struggles with handwriting variations.\n",
    "üîπ **Solution**: A Convolutional Neural Network (CNN) is trained to recognize handwritten text.\n",
    "\n",
    "### Workflow:\n",
    "‚úÖ **Data Collection**: Handwritten samples are collected.\n",
    "‚úÖ **Model Training**: The CNN is trained on thousands of handwritten examples.\n",
    "‚úÖ **Deployment**: Used in mobile banking apps for check deposits.\n",
    "\n",
    "üí° **Result**: AI-powered OCR improves accuracy in reading handwritten text, reducing manual data entry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10a7f0",
   "metadata": {},
   "source": [
    "### Simple Artificial Neuron Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1337ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.6456563062257954\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define input, weights, and bias\n",
    "inputs = np.array([1, 0])  # Binary input\n",
    "weights = np.array([0.5, -0.5])  # Weight coefficients\n",
    "bias = 0.1  # Bias term\n",
    "\n",
    "# Compute output\n",
    "output = sigmoid(np.dot(inputs, weights) + bias)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba871ede",
   "metadata": {},
   "source": [
    "This code simulates a simple artificial neuron:\n",
    "\n",
    "- **Imports** NumPy for numerical operations.\n",
    "- **Defines** the sigmoid function, which squashes values between 0 and 1.\n",
    "- **Initializes** inputs, weights, and bias:\n",
    "  - **Inputs:** `[1, 0]` (binary values).\n",
    "  - **Weights:** `[0.5, -0.5]` (determines feature importance).\n",
    "  - **Bias:** `0.1` (shifts activation threshold).\n",
    "- **Computes** the weighted sum using the dot product of inputs and weights, then adds the bias.\n",
    "- **Applies** the sigmoid function to transform the result into a probability-like value.\n",
    "- **Prints** the final output, which determines neuron activation.\n",
    "\n",
    "This is a basic building block of neural networks. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b733f9",
   "metadata": {},
   "source": [
    "### Simple Neural Network Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b550af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define an MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(4, 1)  # Hidden layer to output layer\n",
    "        self.sigmoid = nn.Sigmoid()  # Output activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = MLP()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d99e63",
   "metadata": {},
   "source": [
    "##### Importing Necessary Libraries:\n",
    "- **`torch`**: The main library for working with deep learning in Python.  \n",
    "- **`torch.nn`**: Contains tools to build neural networks.  \n",
    "- **`torch.optim`**: Used for training the model (**not used in this code**).  \n",
    "\n",
    "---\n",
    "\n",
    "##### Defining the Neural Network (MLP Model):\n",
    "- The `MLP` class is created to define the neural network.  \n",
    "- It **inherits** from `nn.Module`, which is required for PyTorch models.  \n",
    "- The `__init__` method sets up **two layers**:  \n",
    "  - **`fc1`**: Connects **2 input numbers** to **4 neurons** in a hidden layer.  \n",
    "  - **`fc2`**: Connects **4 hidden neurons** to **1 output neuron**.  \n",
    "- The model uses:  \n",
    "  - **ReLU activation function** to allow the model to learn better.  \n",
    "  - **Sigmoid activation function** to make sure the output is between `0` and `1` (**useful for yes/no decisions**).  \n",
    "\n",
    "---\n",
    "\n",
    "#### How the Model Processes Data (`forward` Method):\n",
    "1. The input is passed through **`fc1`**, then the **ReLU activation** is applied.  \n",
    "2. It is then passed through **`fc2`**, followed by **sigmoid activation**.  \n",
    "3. This defines **how the model transforms the input into an output**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Creating and Printing the Model:\n",
    "```python\n",
    "model = MLP()  # Creates the neural network\n",
    "print(model)  # Displays the structure of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d0110",
   "metadata": {},
   "source": [
    "### XOR Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6442cc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6961403489112854\n",
      "Epoch 200, Loss: 0.22329354286193848\n",
      "Epoch 400, Loss: 0.04864644259214401\n",
      "Epoch 600, Loss: 0.019203947857022285\n",
      "Epoch 800, Loss: 0.010208996012806892\n",
      "Final model output: tensor([[0.0055],\n",
      "        [0.9953],\n",
      "        [0.9897],\n",
      "        [0.0047]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample training data\n",
    "X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])  # XOR dataset labels\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)  \n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final model output:\", model(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8b033",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## **1. What the Code Does**\n",
    "This code **trains a neural network** to learn the **XOR function**, a basic logic operation where:\n",
    "\n",
    "- `0 XOR 0 = 0`\n",
    "- `0 XOR 1 = 1`\n",
    "- `1 XOR 0 = 1`\n",
    "- `1 XOR 1 = 0`\n",
    "\n",
    "### **Step-by-Step Breakdown**  \n",
    "\n",
    "### **1.1 Define Loss Function & Optimizer**  \n",
    "- The **Binary Cross-Entropy Loss (BCELoss)** measures how well the model's predictions match actual outputs.  \n",
    "- **Adam optimizer** updates the model‚Äôs weights to minimize the loss and improve predictions.\n",
    "\n",
    "### **1.2 Define the Training Data (XOR Dataset)**  \n",
    "- The input `X_train` consists of 4 possible **binary combinations** (0s and 1s).  \n",
    "- The expected output `y_train` follows the **XOR truth table**.\n",
    "\n",
    "### **1.3 Training the Model (1000 Epochs)**  \n",
    "- The model **predicts outputs**, calculates **loss**, and **adjusts weights** using **gradient descent**.  \n",
    "- Loss is printed every **200 epochs** to track improvement.  \n",
    "\n",
    "### **1.4 Model Evaluation**  \n",
    "- After training, the model makes predictions for `X_train`, and the results are displayed.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Understanding the Output**  \n",
    "The model prints loss values and final predictions. Let's analyze them:\n",
    "\n",
    "### **2.1 Loss at Different Epochs**\n",
    "\n",
    "| Epoch | Loss Value  | Meaning |\n",
    "|--------|-----------|---------|\n",
    "| **0** | **0.6961** | High loss ‚Üí Model is guessing randomly. |\n",
    "| **200** | **0.2233** | Loss decreasing ‚Üí Model is learning the XOR pattern. |\n",
    "| **400** | **0.0486** | Loss is low ‚Üí Predictions are getting close to actual values. |\n",
    "| **600** | **0.0192** | Model has almost mastered XOR. |\n",
    "| **800** | **0.0102** | Very low loss ‚Üí Model is making nearly perfect predictions. |\n",
    "\n",
    "üìå **Key Takeaway:** The model starts with a high loss but gradually improves as it learns from the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Final Model Predictions**\n",
    "```python\n",
    " tensor([[0.0055],  \n",
    "        [0.9953],  \n",
    "        [0.9897],  \n",
    "        [0.0047]])\n",
    "```\n",
    "These values represent the model‚Äôs predicted **probabilities** (between 0 and 1) after applying the **Sigmoid activation function**.\n",
    "\n",
    "| Input (X_train) | Expected Output (y_train) | Model Prediction |\n",
    "|---------------|----------------|----------------|\n",
    "| `[0, 0]`     | `0`            | **0.0055** (Very close to 0 ‚úÖ) |\n",
    "| `[0, 1]`     | `1`            | **0.9953** (Very close to 1 ‚úÖ) |\n",
    "| `[1, 0]`     | `1`            | **0.9897** (Very close to 1 ‚úÖ) |\n",
    "| `[1, 1]`     | `0`            | **0.0047** (Very close to 0 ‚úÖ) |\n",
    "\n",
    "‚úÖ **The model has successfully learned the XOR function!**  \n",
    "- Predictions are **very close** to the expected values.  \n",
    "- It **correctly classifies XOR outputs** after training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefbc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
