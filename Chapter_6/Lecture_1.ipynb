{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e183f15a",
   "metadata": {},
   "source": [
    "## Lecture 1: Sequence-to-Sequence Learning with Transformers\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Sequence-to-sequence (Seq2Seq) learning is a crucial framework for various NLP tasks such as:\n",
    "- Machine translation\n",
    "- Text summarization\n",
    "- Dialogue generation\n",
    "\n",
    "Traditional Seq2Seq models relied on:\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "- **Long Short-Term Memory (LSTMs)**\n",
    "- **Gated Recurrent Units (GRUs)**\n",
    "\n",
    "However, these models suffered from:\n",
    "- **Vanishing gradients**\n",
    "- **Difficulty capturing long-range dependencies**\n",
    "- **Sequential processing constraints** (inefficient for large-scale applications)\n",
    "\n",
    "The **Transformer architecture**, introduced by Vaswani et al. (2017) in *Attention Is All You Need*, revolutionized Seq2Seq learning by:\n",
    "- Replacing RNNs with **self-attention mechanisms** and **positional encoding**\n",
    "- Enabling **parallelized training**\n",
    "- Improving **long-range dependency modeling**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. How Sequence-to-Sequence Learning Works in Transformers\n",
    "\n",
    "Seq2Seq models in Transformers consist of two main components:\n",
    "\n",
    "1. **Encoder**: Processes the input sequence and generates contextual representations.\n",
    "2. **Decoder**: Uses the encoder’s representations to generate an output sequence step by step.\n",
    "\n",
    "Unlike RNN-based models that encode the entire input into a single fixed-size vector, Transformer-based Seq2Seq models use a **multi-layer attention-based architecture**, allowing the decoder to **attend to every word in the input sequence at each step**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Transformer Encoder-Decoder Architecture\n",
    "\n",
    "### 2.1 The Encoder\n",
    "The encoder converts input sequences into meaningful representations by passing tokens through multiple layers of:\n",
    "\n",
    "- **Self-Attention Mechanisms**: Captures relationships between words, even those far apart.\n",
    "- **Feedforward Networks**: Further processes attention outputs.\n",
    "- **Positional Encoding**: Adds order information since Transformers process words in parallel.\n",
    "\n",
    "Each encoder layer follows this structure:\n",
    "\n",
    "1. **Multi-head self-attention**\n",
    "2. **Add & Norm** (residual connection + layer normalization)\n",
    "3. **Feedforward network** (fully connected layers)\n",
    "4. **Add & Norm**\n",
    "\n",
    "### 2.2 The Decoder\n",
    "The decoder generates output sequences **one token at a time**, using:\n",
    "\n",
    "- **Masked Multi-Head Attention**: Ensures tokens can only attend to past words to prevent cheating.\n",
    "- **Cross-Attention Mechanism**: Attends to encoder outputs to incorporate input sequence information.\n",
    "- **Feedforward Networks**: Refines the generated representations.\n",
    "\n",
    "The decoder follows this structure:\n",
    "\n",
    "1. **Masked multi-head self-attention**\n",
    "2. **Add & Norm**\n",
    "3. **Encoder-decoder cross-attention**\n",
    "4. **Add & Norm**\n",
    "5. **Feedforward network**\n",
    "6. **Add & Norm**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Innovations of Transformer-Based Seq2Seq Models\n",
    "\n",
    "### 3.1 Self-Attention and Cross-Attention\n",
    "- The **self-attention mechanism** allows the model to dynamically weigh the importance of different words.\n",
    "- **Cross-attention** ensures that the decoder properly references the encoded input.\n",
    "\n",
    "### 3.2 Positional Encoding\n",
    "- Since Transformers do not process sequences in order, they require **positional encodings** (sine and cosine functions) to encode word order information.\n",
    "\n",
    "### 3.3 Parallelization\n",
    "- Unlike RNN-based models that process input tokens sequentially, **Transformers use parallel computation**, making them significantly faster and more scalable.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Sequence-to-Sequence Transformer Models\n",
    "\n",
    "### 4.1 T5 (Text-to-Text Transfer Transformer)\n",
    "- Treats all NLP tasks as **text-to-text problems** (e.g., translation, summarization, Q&A).\n",
    "- Uses **denoising pretraining**, where it reconstructs corrupted text.\n",
    "- Supports **multi-task learning**, handling multiple NLP tasks with a unified framework.\n",
    "\n",
    "### 4.2 BART (Bidirectional and Auto-Regressive Transformer)\n",
    "- Combines **BERT’s bidirectional understanding** with **GPT’s autoregressive generation**.\n",
    "- Excellent for **text summarization** and **machine translation**.\n",
    "- Uses **denoising objectives** to improve robustness.\n",
    "\n",
    "### 4.3 PEGASUS\n",
    "- Specialized for **text summarization** using **gap-sentence pretraining**.\n",
    "- Selects and masks entire key sentences, forcing the model to generate them from context.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training a Sequence-to-Sequence Transformer\n",
    "\n",
    "### Step 1: Data Preprocessing\n",
    "- Tokenize input/output sequences.\n",
    "- Add special tokens (e.g., `[CLS]`, `[SEP]`).\n",
    "- Convert text to numerical embeddings.\n",
    "\n",
    "### Step 2: Model Training\n",
    "- Use **CrossEntropyLoss** to compare predicted and actual tokens.\n",
    "- Apply **teacher forcing** during training (feeding correct tokens to the decoder).\n",
    "- Optimize with **AdamW optimizer**.\n",
    "\n",
    "### Step 3: Inference (Generating Text)\n",
    "- Use **Greedy Decoding** (selecting the highest probability token).\n",
    "- Use **Beam Search** for more fluent generation.\n",
    "- Use **Top-k Sampling** for creative output.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advantages of Transformer-Based Seq2Seq Learning\n",
    "\n",
    "✅ Handles **long-range dependencies** better than RNNs.  \n",
    "✅ Allows for **parallel computation**, making training faster.  \n",
    "✅ Achieves **state-of-the-art results** in NLP tasks like translation and summarization.  \n",
    "✅ **Scalable** to large datasets and complex applications.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e3348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
