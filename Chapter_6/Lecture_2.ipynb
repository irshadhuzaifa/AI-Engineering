{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd810b5",
   "metadata": {},
   "source": [
    "## Lecture 2: Transformers in NLP – Text Generation, Summarization, and Translation\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Transformer architecture has revolutionized NLP by enabling models to generate, summarize, and translate text more effectively than traditional approaches like RNNs and LSTMs. With the introduction of self-attention and parallel processing, Transformers have significantly improved the quality, fluency, and speed of text-based applications.\n",
    "\n",
    "This lecture explores three major applications of Transformers in NLP:\n",
    "\n",
    "- **Text Generation** – Autoregressive models like GPT create human-like text.\n",
    "- **Summarization** – Encoder-decoder models like BART generate concise summaries.\n",
    "- **Translation** – Seq2Seq models like T5 and mT5 enable high-quality translations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Text Generation with Transformers\n",
    "\n",
    "### How Transformers Generate Text\n",
    "\n",
    "Text generation is typically performed by decoder-only Transformers, such as the GPT family (GPT-2, GPT-3, GPT-4), which use autoregressive language modeling. These models generate text one token at a time, predicting the next token based on previously generated tokens.\n",
    "\n",
    "### Key Features of Transformer-Based Text Generation\n",
    "\n",
    "- **Autoregressive Modeling:** Uses past tokens to predict the next token.\n",
    "- **Few-Shot and Zero-Shot Learning:** GPT models can generate high-quality responses with minimal training on new tasks.\n",
    "- **Controllable Output:** By using prompts, models can be guided to generate structured text.\n",
    "\n",
    "### Popular Transformer Models for Text Generation\n",
    "\n",
    "| Model    | Architecture       | Use Case                           |\n",
    "|----------|-------------------|------------------------------------|\n",
    "| GPT-2    | Decoder-only      | Creative writing, story generation |\n",
    "| GPT-3/4  | Decoder-only      | Conversational AI, coding, essays  |\n",
    "| XLNet    | Autoregressive & Bidirectional | Text prediction, improved coherence |\n",
    "| CTRL     | Conditional Text Generation | Controlled text generation with specific styles |\n",
    "\n",
    "### Example: Using GPT for Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9726cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, our country is plagued by endless conflicts over territory, including the borders of the US, Europe, and the Middle East.\n",
      "\n",
      "President Donald Trump was not on the same page at all, with many Democrats citing the Russian annexation of Crimea and the U.S. attack.\n",
      "\n",
      "In a statement issued Wednesday, White House press secretary Sean Spicer said \"there are no issues\" over the White House's decision to not issue a travel ban, despite the fact Russia and its Foreign Minister Lavrov had warned of \"significant potential\" repercussions.\n",
      "\n",
      "\"The decision will have a very significant impact within the context of our country and our security. Those who want to travel should be welcome,\" he said.\n",
      "\n",
      "\"Our country should be welcoming all those who wish to come to our country, and also be respectful of those Americans who have been murdered or tortured on a foreign soil,\" the statement continued. \"We want to ensure that our border is not taken over by people who\n",
      "\n",
      "Prompt: In a world where technology revolutionized everything,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: In a world where technology revolutionized everything, the internet is becoming a global issue. And while I believe that there will always be more people with more options online, the challenges of moving to digital platforms like Facebook and Twitter, and more freedom from censorship continue to define us.\n",
      "\n",
      "We'll continue to work to put a human face into our world. But let's not overdo the message by assuming we've built an infinite computer system. For this reason, the Internet is a powerful tool to push us to new heights of understanding about the future of all people.\n",
      "\n",
      "Today in the world, there is an incredible amount of science to understand how our brains are capable of understanding everything we do, and how we can improve our lives and our ability to live. But we don't have the technology to create a computerized world. The real danger is not technology but it's ignorance: the very idea that you can write a computer program in three days, with the same kind of complexity and\n",
      "\n",
      "Prompt: The curious scientist discovered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: The curious scientist discovered that not only was this an astonishing discovery.\n",
      "\n",
      "\"The theory says that if you were to find out if something is wrong with your DNA from inside a person's own person, you would find out,\" he explained.\n",
      "\n",
      "Dr Steve Bove, director of animal services at the Natural History Museum of London, told BBC News: \"This is very much an exciting discovery and a fascinating study. \"What is a known way to tell us if something is an animal?\n",
      "\n",
      "\"We still don't know very much about these organisms, but here we can tell us much about how they function, even if there were no such thing as a human-like organism yet.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import warnings\n",
    "\n",
    "def generate_text(prompt, max_length=150, model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Generate text using a pre-trained language model with improved configuration.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Starting text to generate from\n",
    "        max_length (int): Maximum total length of generated text\n",
    "        model_name (str): Hugging Face model identifier\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    # Suppress specific HuggingFace warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create text generation pipeline with explicit truncation\n",
    "    generator = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=max_length,\n",
    "        truncation=True,  # Explicitly set truncation\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    "    )\n",
    "    \n",
    "    # Generate text\n",
    "    try:\n",
    "        generated_texts = generator(prompt)\n",
    "        return generated_texts[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text generation: {e}\")\n",
    "        return prompt\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Different prompts to showcase text generation\n",
    "    prompts = [\n",
    "        \"Once upon a time,\",\n",
    "        \"In a world where technology revolutionized everything,\",\n",
    "        \"The curious scientist discovered\",\n",
    "    ]\n",
    "    \n",
    "    # Generate text for each prompt\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        result = generate_text(prompt, max_length=200)\n",
    "        print(f\"Generated Text: {result}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e937",
   "metadata": {},
   "source": [
    "## 2. Summarization with Transformers\n",
    "\n",
    "### How Transformers Summarize Text\n",
    "\n",
    "Summarization is commonly handled by encoder-decoder Transformer models, such as:\n",
    "\n",
    "- **BART** (Bidirectional and Auto-Regressive Transformer)\n",
    "- **T5** (Text-to-Text Transfer Transformer)\n",
    "- **PEGASUS** (Pretraining with Extracted Gap-Sentences)\n",
    "\n",
    "These models first encode the input text, then decode a shorter version while retaining key information.\n",
    "\n",
    "### Types of Summarization\n",
    "\n",
    "- **Extractive Summarization:** Selects key sentences from the input (e.g., BERTSUM).\n",
    "- **Abstractive Summarization:** Generates a new, shorter version in its own words (e.g., BART, T5).\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **News Summarization** (e.g., Google News)\n",
    "- **Legal Document Summarization**\n",
    "- **Scientific Paper Summarization** (e.g., Semantic Scholar)\n",
    "\n",
    "### Example: Summarization with BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c892d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Summarization Examples:\n",
      "\n",
      "Scientific Text:\n",
      "Original:\n",
      "Transformers have significantly changed the landscape of Natural Language\n",
      "Processing (NLP) by introducing the self-attention mechanism. Unlike traditional\n",
      "Recurrent Neural Networks (RNNs), transformer models can process entire\n",
      "sequences in parallel, leading to more efficient and effective language\n",
      "understanding and generation.\n",
      "\n",
      "Summary:\n",
      "Transformers can process entire sequences in parallel, leading to more efficient\n",
      "and effective language understanding and generation. Transformers have\n",
      "significantly changed the landscape of Natural Language Processing (NLP) by\n",
      "introducing the self-attention mechanism.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "News Article Text:\n",
      "Original:\n",
      "A breakthrough in renewable energy technology has been announced by a team of\n",
      "researchers at Stanford University. The new solar panel design increases energy\n",
      "conversion efficiency by 40% compared to current commercial panels. This\n",
      "innovation could potentially reduce solar energy costs and make sustainable\n",
      "power more accessible to communities worldwide.\n",
      "\n",
      "Summary:\n",
      "A breakthrough in renewable energy technology has been announced. The new solar\n",
      "panel design increases energy conversion efficiency by 40% compared to current\n",
      "commercial panels.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Academic Text:\n",
      "Original:\n",
      "The interdisciplinary study of cognitive neuroscience explores the complex\n",
      "relationships between brain functions and cognitive processes. By integrating\n",
      "methodologies from psychology, biology, and computer science, researchers aim to\n",
      "develop comprehensive models of human perception, memory, and decision-making\n",
      "mechanisms.\n",
      "\n",
      "Summary:\n",
      "Interdisciplinary study of cognitive neuroscience explores the complex\n",
      "relationships between brain functions and cognitive processes. Researchers aim\n",
      "to develop comprehensive models of human perception, memory, and decision-making\n",
      "mechanisms.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from transformers import pipeline\n",
    "\n",
    "def demonstrate_bart_summarization():\n",
    "    # Initialize the BART summarization pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    # Different types of text to summarize\n",
    "    texts = {\n",
    "        \"Scientific\": \"Transformers have significantly changed the landscape of Natural Language Processing (NLP) by introducing the self-attention mechanism. Unlike traditional Recurrent Neural Networks (RNNs), transformer models can process entire sequences in parallel, leading to more efficient and effective language understanding and generation.\",\n",
    "        \n",
    "        \"News Article\": \"A breakthrough in renewable energy technology has been announced by a team of researchers at Stanford University. The new solar panel design increases energy conversion efficiency by 40% compared to current commercial panels. This innovation could potentially reduce solar energy costs and make sustainable power more accessible to communities worldwide.\",\n",
    "        \n",
    "        \"Academic\": \"The interdisciplinary study of cognitive neuroscience explores the complex relationships between brain functions and cognitive processes. By integrating methodologies from psychology, biology, and computer science, researchers aim to develop comprehensive models of human perception, memory, and decision-making mechanisms.\"\n",
    "    }\n",
    "    \n",
    "    # Summarization parameters\n",
    "    summarization_params = {\n",
    "        \"max_length\": 50,   # Maximum length of summary\n",
    "        \"min_length\": 20,   # Minimum length of summary\n",
    "        \"do_sample\": False  # Use deterministic summarization\n",
    "    }\n",
    "    \n",
    "    # Demonstrate summarization for each text\n",
    "    print(\"BART Summarization Examples:\\n\")\n",
    "    for text_type, text in texts.items():\n",
    "        print(f\"{text_type} Text:\")\n",
    "        print(\"Original:\")\n",
    "        print(textwrap.fill(text, width=80))\n",
    "        print(\"\\nSummary:\")\n",
    "        summary = summarizer(text, **summarization_params)[0]['summary_text']\n",
    "        print(textwrap.fill(summary, width=80))\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_bart_summarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c95c6",
   "metadata": {},
   "source": [
    "## 3. Translation with Transformers\n",
    "\n",
    "### How Transformers Handle Machine Translation\n",
    "\n",
    "Machine translation (MT) uses encoder-decoder architectures, where:\n",
    "\n",
    "- The **encoder** processes the source language.\n",
    "- The **decoder** generates text in the target language.\n",
    "\n",
    "### Popular Transformer Models for Translation\n",
    "\n",
    "- **T5:** A general-purpose text-to-text model supporting multilingual translation.\n",
    "- **mT5 (Multilingual T5):** Optimized for translation in 100+ languages.\n",
    "- **M2M-100:** Facebook’s model that translates directly between 100+ languages without relying on English as an intermediate.\n",
    "\n",
    "### Example: Translating Text with mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220a91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (en): Hello, how are you?\n",
      "Translated (fr): Bonjour, comment allez-vous ?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995dc0499b634e169c67bc4820fb9dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\physi\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\physi\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-es. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ad6a674fc743e590070d237ef59564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae58868efc0c43c08f35f6c9998d9f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/819k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84c97abf0140648995ee9ba57e2020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8f14a144714c1980fef8bbfc59b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfee18a7b67b4031932e0aaa8a394645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db622aeb331a45eca1aa74454b906c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb6953dfb48408daac2ac2566296bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (fr): Bonjour le monde\n",
      "Translated (es): Hola, mundo.\n",
      "\n",
      "Original (en): Machine learning is fascinating\n",
      "Translated (de): Maschinelles Lernen ist faszinierend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate_text(text, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    Translate text between languages using Helsinki-NLP's Marian MT models.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be translated\n",
    "        source_lang (str): Source language code (e.g., 'en', 'fr', 'es')\n",
    "        target_lang (str): Target language code\n",
    "    \n",
    "    Returns:\n",
    "        str: Translated text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format language codes for model name\n",
    "        source_lang = source_lang.lower()\n",
    "        target_lang = target_lang.lower()\n",
    "        \n",
    "        # Determine the correct model name based on language pair\n",
    "        if source_lang == \"fr\" and target_lang == \"es\":\n",
    "            # For French to Spanish, we'll use Romance to Spanish model\n",
    "            model_name = \"Helsinki-NLP/opus-mt-fr-es\"  # Direct model\n",
    "        else:\n",
    "            model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Tokenize and translate\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(**inputs, max_length=100)\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return translated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Detailed error: {type(e).__name__} - {str(e)}\")\n",
    "        \n",
    "        # If direct translation failed, try using English as pivot\n",
    "        if source_lang != \"en\" and target_lang != \"en\":\n",
    "            try:\n",
    "                print(f\"Trying two-step translation via English...\")\n",
    "                # First translate to English\n",
    "                en_model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-en\"\n",
    "                en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
    "                en_model = MarianMTModel.from_pretrained(en_model_name)\n",
    "                \n",
    "                # Source → English\n",
    "                en_inputs = en_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "                en_translated = en_model.generate(**en_inputs, max_length=100)\n",
    "                english_text = en_tokenizer.batch_decode(en_translated, skip_special_tokens=True)[0]\n",
    "                \n",
    "                # English → Target\n",
    "                target_model_name = f\"Helsinki-NLP/opus-mt-en-{target_lang}\"\n",
    "                target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "                target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "                \n",
    "                # English → Target\n",
    "                target_inputs = target_tokenizer(english_text, return_tensors=\"pt\", padding=True)\n",
    "                final_translated = target_model.generate(**target_inputs, max_length=100)\n",
    "                final_text = target_tokenizer.batch_decode(final_translated, skip_special_tokens=True)[0]\n",
    "                \n",
    "                return final_text\n",
    "            except Exception as pivot_error:\n",
    "                return f\"Translation failed: {str(e)}. Pivot translation also failed: {str(pivot_error)}\"\n",
    "        \n",
    "        return f\"Translation failed: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    # Translation examples\n",
    "    translations = [\n",
    "        {\"text\": \"Hello, how are you?\", \"source\": \"en\", \"target\": \"fr\"},\n",
    "        {\"text\": \"Bonjour le monde\", \"source\": \"fr\", \"target\": \"es\"},\n",
    "        {\"text\": \"Machine learning is fascinating\", \"source\": \"en\", \"target\": \"de\"}\n",
    "    ]\n",
    "    \n",
    "    # Perform translations\n",
    "    for translation in translations:\n",
    "        result = translate_text(\n",
    "            translation['text'], \n",
    "            translation['source'], \n",
    "            translation['target']\n",
    "        )\n",
    "        print(f\"Original ({translation['source']}): {translation['text']}\")\n",
    "        print(f\"Translated ({translation['target']}): {result}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e35c9b",
   "metadata": {},
   "source": [
    "### Advancements in Translation\n",
    "\n",
    "- **Zero-Shot Translation:** Models can translate unseen language pairs.\n",
    "- **Massive Multilingual Training:** Allows translation across 100+ languages.\n",
    "- **Cross-Lingual Knowledge Sharing:** Improves translation quality by leveraging multiple languages in training.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Challenges and Future Directions\n",
    "\n",
    "While Transformers have made breakthroughs, challenges remain:\n",
    "\n",
    "- ✅ **Computational Costs:** Large models require high memory and processing power.\n",
    "- ✅ **Bias and Hallucinations:** Models can generate inaccurate translations or biased summaries.\n",
    "- ✅ **Handling Low-Resource Languages:** Many languages lack sufficient training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f97899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
