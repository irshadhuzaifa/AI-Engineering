{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3571d0",
   "metadata": {},
   "source": [
    "# Lecture 1: LoRA, QLoRA, and Other Lightweight Fine-Tuning Approaches\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As large language models (LLMs) grow in size, traditional fine-tuning approaches become computationally expensive and impractical for many users. LoRA (Low-Rank Adaptation), QLoRA (Quantized LoRA), and other lightweight fine-tuning methods offer an efficient way to adapt models while drastically reducing memory and compute requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "LoRA was introduced to address the inefficiencies of full fine-tuning by reducing the number of trainable parameters. Instead of updating the entire model, LoRA:\n",
    "\n",
    "- Freezes the original model parameters to maintain general knowledge.\n",
    "- Introduces low-rank matrices that capture fine-tuning adjustments.\n",
    "- Merges these learned matrices back into the base model after training.\n",
    "\n",
    "### Mathematical Concept\n",
    "\n",
    "Given a weight matrix $W$ of dimension $n \\times m$, LoRA replaces it with:\n",
    "\n",
    "$$W' = W + \\Delta W$$\n",
    "\n",
    "Where $\\Delta W$ is decomposed into two smaller matrices:\n",
    "\n",
    "$$\\Delta W = A B$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A$ is an $n \\times r$ matrix.\n",
    "- $B$ is an $r \\times m$ matrix.\n",
    "- $r$ (LoRA rank) controls the compression.\n",
    "\n",
    "### Advantages of LoRA\n",
    "\n",
    "- Reduces computational cost by updating only a small number of parameters.\n",
    "- Minimizes memory overhead, making fine-tuning feasible on consumer GPUs.\n",
    "- Enables task-specific customization without losing general model knowledge.\n",
    "\n",
    "### Challenges of LoRA\n",
    "\n",
    "- Limited flexibility in certain domains compared to full fine-tuning.\n",
    "- Requires integration into existing transformer architectures, which can be challenging for less popular models.\n",
    "\n",
    "---\n",
    "\n",
    "## QLoRA (Quantized LoRA)\n",
    "\n",
    "### How QLoRA Works\n",
    "\n",
    "QLoRA builds on LoRA by applying 4-bit quantization to reduce memory usage further. The key innovations include:\n",
    "\n",
    "- Quantizing model weights to 4-bit NormalFloat-4 (NF4) format for efficient storage.\n",
    "- Using paged optimizers to dynamically offload data between CPU and GPU.\n",
    "- Applying LoRA on top of quantized weights, combining memory efficiency with fine-tuning flexibility.\n",
    "\n",
    "### Performance of QLoRA\n",
    "\n",
    "QLoRA enables fine-tuning a 65B-parameter model on a single 48GB GPU, which was previously impractical. Benchmarks have shown that QLoRA-based models like Guanaco achieve competitive performance against models such as GPT-4 and ChatGPT.\n",
    "\n",
    "### Advantages of QLoRA\n",
    "\n",
    "- Massive memory savings (~4x compared to standard LoRA).\n",
    "- Allows fine-tuning large models on consumer-grade hardware.\n",
    "- Retains most of the accuracy of full fine-tuning.\n",
    "\n",
    "### Challenges of QLoRA\n",
    "\n",
    "- Increased training time due to quantization and dequantization overhead.\n",
    "- Precision loss in some applications where high numerical accuracy is required.\n",
    "\n",
    "---\n",
    "\n",
    "## Other Lightweight Fine-Tuning Approaches\n",
    "\n",
    "### AdaLoRA (Adaptive LoRA)\n",
    "\n",
    "- Improves LoRA by allocating different ranks dynamically to different model layers.\n",
    "- Reduces redundancy by pruning less important parameters.\n",
    "\n",
    "### QA-LoRA, ModuLoRA, and IR-QLoRA\n",
    "\n",
    "- Variants of QLoRA that optimize quantization strategies for different applications.\n",
    "- **QA-LoRA:** Focuses on question-answering tasks.\n",
    "- **ModuLoRA:** Applies modular fine-tuning to specific layers.\n",
    "- **IR-QLoRA:** Enhances inference efficiency.\n",
    "\n",
    "### Soft Prompt Tuning\n",
    "\n",
    "- Instead of modifying model weights, learns trainable embeddings (soft prompts) that influence model behavior.\n",
    "- Efficient for multi-task learning and applications where full fine-tuning is impractical.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "LoRA, QLoRA, and other parameter-efficient fine-tuning (PEFT) approaches have transformed the way we adapt large AI models. They balance efficiency, memory savings, and performance, making fine-tuning accessible to a broader audience. The choice between these techniques depends on hardware constraints, model size, and the specific application. ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dece57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
