{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe23aa9",
   "metadata": {},
   "source": [
    "## Lecture 5: Hands-On: Implementing a Vector Database for AI\n",
    "\n",
    "### Introduction\n",
    "In this hands-on lecture, we will implement a vector database for an AI application. We'll focus on how to convert unstructured data, such as text or images, into vectors, and store them in a vector database for efficient similarity-based search. By the end of this lecture, you will have a functional vector database that can be used to store, retrieve, and search for similar data points based on their vector representations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import time\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector database with a specific transformer model.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the pre-trained model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of strings to embed\n",
    "\n",
    "        Returns:\n",
    "            NumPy array of embeddings\n",
    "        \"\"\"\n",
    "        # Tokenize and encode the texts\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True,\n",
    "                                max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use mean of last hidden states as embedding\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def add_documents(self, documents: List[str], metadata: Optional[List[Dict[str, Any]]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the database and build or update the index.\n",
    "\n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            metadata: Optional list of metadata dictionaries for each document\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            print(\"No documents provided\")\n",
    "            return\n",
    "\n",
    "        # Generate embeddings for the documents\n",
    "        print(f\"Generating embeddings for {len(documents)} documents...\")\n",
    "        start_time = time.time()\n",
    "        embeddings = self.generate_embeddings(documents)\n",
    "        print(f\"Embeddings generated in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        # Create or update the index\n",
    "        if self.index is None:\n",
    "            dimension = embeddings.shape[1]\n",
    "            print(f\"Creating new FAISS index with dimension {dimension}\")\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "        # Add embeddings to the index\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        # Store the documents and metadata\n",
    "        start_idx = len(self.documents)\n",
    "        self.documents.extend(documents)\n",
    "\n",
    "        # Add metadata if provided, otherwise use empty dictionaries\n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in documents]\n",
    "        assert len(metadata) == len(documents), \"Metadata list must match documents list length\"\n",
    "\n",
    "        # Add document index to metadata\n",
    "        for i, meta in enumerate(metadata):\n",
    "            meta['document_idx'] = start_idx + i\n",
    "\n",
    "        self.metadata.extend(metadata)\n",
    "\n",
    "        print(f\"Added {len(documents)} documents to index. Total documents: {len(self.documents)}\")\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar documents based on a query string.\n",
    "\n",
    "        Args:\n",
    "            query: Query string to search for\n",
    "            k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing search results\n",
    "        \"\"\"\n",
    "        if self.index is None or len(self.documents) == 0:\n",
    "            return []\n",
    "\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.generate_embeddings([query])\n",
    "\n",
    "        # Perform the search\n",
    "        k = min(k, len(self.documents))  # Ensure k is not larger than number of documents\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "\n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "            result = {\n",
    "                'rank': i + 1,\n",
    "                'document_idx': int(idx),\n",
    "                'distance': float(distance),\n",
    "                'text': self.documents[idx],\n",
    "                'metadata': self.metadata[idx]\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save(self, directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the vector database to disk.\n",
    "\n",
    "        Args:\n",
    "            directory: Directory to save the database\n",
    "        \"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Save the index\n",
    "        if self.index is not None:\n",
    "            faiss.write_index(self.index, os.path.join(directory, \"index.faiss\"))\n",
    "\n",
    "        # Save documents and metadata\n",
    "        with open(os.path.join(directory, \"documents.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.documents, f)\n",
    "\n",
    "        with open(os.path.join(directory, \"metadata.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "\n",
    "        # Save model name\n",
    "        with open(os.path.join(directory, \"model_name.txt\"), 'w') as f:\n",
    "            f.write(self.model_name)\n",
    "\n",
    "        print(f\"Vector database saved to {directory}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, directory: str) -> 'VectorDatabase':\n",
    "        \"\"\"\n",
    "        Load a vector database from disk.\n",
    "\n",
    "        Args:\n",
    "            directory: Directory containing the saved database\n",
    "\n",
    "        Returns:\n",
    "            Loaded VectorDatabase instance\n",
    "        \"\"\"\n",
    "        # Load model name\n",
    "        with open(os.path.join(directory, \"model_name.txt\"), 'r') as f:\n",
    "            model_name = f.read().strip()\n",
    "\n",
    "        # Create instance with the same model\n",
    "        db = cls(model_name)\n",
    "\n",
    "        # Load documents and metadata\n",
    "        with open(os.path.join(directory, \"documents.pkl\"), 'rb') as f:\n",
    "            db.documents = pickle.load(f)\n",
    "\n",
    "        with open(os.path.join(directory, \"metadata.pkl\"), 'rb') as f:\n",
    "            db.metadata = pickle.load(f)\n",
    "\n",
    "        # Load the index\n",
    "        index_path = os.path.join(directory, \"index.faiss\")\n",
    "        if os.path.exists(index_path):\n",
    "            db.index = faiss.read_index(index_path)\n",
    "\n",
    "        print(f\"Loaded vector database from {directory} with {len(db.documents)} documents\")\n",
    "        return db\n",
    "\n",
    "\n",
    "def batch_process_documents(documents: List[str], batch_size: int = 8) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process a large number of documents in batches to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document texts\n",
    "        batch_size: Number of documents to process at once\n",
    "\n",
    "    Returns:\n",
    "        List of embeddings for all documents\n",
    "    \"\"\"\n",
    "    db = VectorDatabase()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{(len(documents) - 1) // batch_size + 1}\")\n",
    "        embeddings = db.generate_embeddings(batch)\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage of the VectorDatabase class\n",
    "\n",
    "    # Create a new vector database\n",
    "    db = VectorDatabase()\n",
    "\n",
    "    # Example documents\n",
    "    documents = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"A journey of a thousand miles begins with a single step.\",\n",
    "        \"To be or not to be, that is the question.\",\n",
    "        \"All that glitters is not gold.\",\n",
    "        \"The early bird catches the worm.\",\n",
    "        \"Actions speak louder than words.\",\n",
    "        \"Don't judge a book by its cover.\",\n",
    "        \"The pen is mightier than the sword.\",\n",
    "        \"Fortune favors the bold.\",\n",
    "        \"Knowledge is power.\"\n",
    "    ]\n",
    "\n",
    "    # Add metadata for each document\n",
    "    metadata = [\n",
    "        {\"source\": \"proverb\", \"category\": \"animals\"},\n",
    "        {\"source\": \"Lao Tzu\", \"category\": \"philosophy\"},\n",
    "        {\"source\": \"Shakespeare\", \"category\": \"literature\"},\n",
    "        {\"source\": \"proverb\", \"category\": \"wisdom\"},\n",
    "        {\"source\": \"proverb\", \"category\": \"animals\"},\n",
    "        {\"source\": \"proverb\", \"category\": \"behavior\"},\n",
    "        {\"source\": \"proverb\", \"category\": \"wisdom\"},\n",
    "        {\"source\": \"Edward Bulwer-Lytton\", \"category\": \"literature\"},\n",
    "        {\"source\": \"proverb\", \"category\": \"courage\"},\n",
    "        {\"source\": \"Francis Bacon\", \"category\": \"wisdom\"}\n",
    "    ]\n",
    "\n",
    "    # Add documents to the database\n",
    "    db.add_documents(documents, metadata)\n",
    "\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"What is the meaning of life?\",\n",
    "        \"Tell me about courage and boldness\",\n",
    "        \"I need some wisdom about appearances\"\n",
    "    ]\n",
    "\n",
    "    # Search for each query\n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = db.search(query)\n",
    "\n",
    "        print(\"Search Results:\")\n",
    "        for result in results:\n",
    "            print(f\"{result['rank']}. (Distance: {result['distance']:.4f}) '{result['text']}'\")\n",
    "            print(\n",
    "                f\"   Source: {result['metadata'].get('source', 'Unknown')}, Category: {result['metadata'].get('category', 'Uncategorized')}\")\n",
    "\n",
    "    # Save the database\n",
    "    save_dir = \"vector_db_example\"\n",
    "    db.save(save_dir)\n",
    "\n",
    "    # Load the database\n",
    "    loaded_db = VectorDatabase.load(save_dir)\n",
    "\n",
    "    # Verify the loaded database works\n",
    "    print(\"\\nTesting loaded database:\")\n",
    "    results = loaded_db.search(\"Tell me about wisdom\")\n",
    "\n",
    "    print(\"Search Results:\")\n",
    "    for result in results:\n",
    "        print(f\"{result['rank']}. (Distance: {result['distance']:.4f}) '{result['text']}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d91cb3",
   "metadata": {},
   "source": [
    "### Output:\n",
    "```\n",
    "Loading model: distilbert-base-uncased\n",
    "Generating embeddings for 10 documents...\n",
    "Embeddings generated in 0.44 seconds\n",
    "Creating new FAISS index with dimension 768\n",
    "Added 10 documents to index. Total documents: 10\n",
    "\n",
    "Query: What is the meaning of life?\n",
    "Search Results:\n",
    "1. (Distance: 28.9064) 'Knowledge is power.'\n",
    "   Source: Francis Bacon, Category: wisdom\n",
    "2. (Distance: 29.8704) 'To be or not to be, that is the question.'\n",
    "   Source: Shakespeare, Category: literature\n",
    "3. (Distance: 35.8834) 'Actions speak louder than words.'\n",
    "   Source: proverb, Category: behavior\n",
    "4. (Distance: 37.3288) 'All that glitters is not gold.'\n",
    "   Source: proverb, Category: wisdom\n",
    "5. (Distance: 39.9361) 'A journey of a thousand miles begins with a single step.'\n",
    "   Source: Lao Tzu, Category: philosophy\n",
    "\n",
    "Query: Tell me about courage and boldness\n",
    "Search Results:\n",
    "1. (Distance: 29.0239) 'Knowledge is power.'\n",
    "   Source: Francis Bacon, Category: wisdom\n",
    "2. (Distance: 29.1098) 'Actions speak louder than words.'\n",
    "   Source: proverb, Category: behavior\n",
    "3. (Distance: 32.2485) 'Fortune favors the bold.'\n",
    "   Source: proverb, Category: courage\n",
    "4. (Distance: 33.7360) 'The pen is mightier than the sword.'\n",
    "   Source: Edward Bulwer-Lytton, Category: literature\n",
    "5. (Distance: 34.9861) 'Don't judge a book by its cover.'\n",
    "   Source: proverb, Category: wisdom\n",
    "\n",
    "Query: I need some wisdom about appearances\n",
    "Search Results:\n",
    "1. (Distance: 26.9088) 'Don't judge a book by its cover.'\n",
    "   Source: proverb, Category: wisdom\n",
    "2. (Distance: 28.1751) 'Actions speak louder than words.'\n",
    "   Source: proverb, Category: behavior\n",
    "3. (Distance: 29.9099) 'Knowledge is power.'\n",
    "   Source: Francis Bacon, Category: wisdom\n",
    "4. (Distance: 31.9937) 'Fortune favors the bold.'\n",
    "   Source: proverb, Category: courage\n",
    "5. (Distance: 34.1386) 'The pen is mightier than the sword.'\n",
    "   Source: Edward Bulwer-Lytton, Category: literature\n",
    "Vector database saved to vector_db_example\n",
    "Loading model: distilbert-base-uncased\n",
    "Loaded vector database from vector_db_example with 10 documents\n",
    "\n",
    "Testing loaded database:\n",
    "Search Results:\n",
    "1. (Distance: 30.5364) 'Knowledge is power.'\n",
    "2. (Distance: 32.7921) 'Don't judge a book by its cover.'\n",
    "3. (Distance: 33.9872) 'Actions speak louder than words.'\n",
    "4. (Distance: 35.6948) 'Fortune favors the bold.'\n",
    "5. (Distance: 35.7062) 'All that glitters is not gold.'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c490a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
