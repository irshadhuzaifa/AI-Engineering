{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17be3c7f",
   "metadata": {},
   "source": [
    "# Lecture 3: Multi-Head Attention and Positional Encoding\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Transformer models, **multi-head attention** and **positional encoding** are two critical mechanisms that enable the model to process input sequences efficiently and capture complex dependencies. \n",
    "- **Multi-head attention** enhances the ability of self-attention by allowing multiple attention mechanisms to run in parallel.\n",
    "- **Positional encoding** provides the necessary sequence order information that is otherwise missing in Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "### What is Multi-Head Attention?\n",
    "\n",
    "Multi-head attention is an extension of the **self-attention** mechanism that improves the model’s ability to capture different types of relationships between words. Instead of applying a single attention function, multiple attention heads operate in parallel, each learning distinct contextual information.\n",
    "\n",
    "### Process of Multi-Head Attention\n",
    "\n",
    "1. **Linear Transformation**: The input embeddings are projected into **Query (Q), Key (K), and Value (V)** matrices using learned weight matrices.\n",
    "2. **Splitting into Multiple Heads**: The Q, K, and V matrices are divided into multiple smaller sub-matrices, each corresponding to an individual attention head.\n",
    "3. **Scaled Dot-Product Attention**: Each head independently computes attention scores, applies softmax normalization, and generates an attention-weighted output.\n",
    "4. **Concatenation and Projection**: The outputs of all attention heads are concatenated and passed through a final linear transformation to integrate the information.\n",
    "\n",
    "This approach allows the model to focus on multiple aspects of a sentence simultaneously, improving comprehension of complex relationships.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Each attention head operates independently using the **scaled dot-product attention**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( Q \\) (Query), \\( K \\) (Key), and \\( V \\) (Value) are projections of input embeddings.\n",
    "- \\( d_k \\) is the dimension of the key vectors.\n",
    "- The **softmax** function ensures that attention scores sum to 1.\n",
    "\n",
    "For **multi-head attention**, multiple sets of \\( Q, K, V \\) matrices are computed:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "where each head is computed using the **scaled dot-product attention** formula.\n",
    "\n",
    "### Benefits of Multi-Head Attention\n",
    "\n",
    "- **Captures different linguistic patterns**: Different heads learn different types of relationships (e.g., syntactic vs. semantic dependencies).\n",
    "- **Improves model expressiveness**: By splitting attention across multiple perspectives, the model captures a richer representation.\n",
    "- **Enhances parallelization**: Multiple attention heads operate independently, making the model efficient on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "### Why is Positional Encoding Needed?\n",
    "\n",
    "Unlike **RNNs**, which process sequences step by step, **Transformers** process entire sequences in parallel. This means Transformers do not inherently understand word order, which is crucial for language understanding.\n",
    "\n",
    "To address this, **positional encoding** is added to the input embeddings to incorporate word order information.\n",
    "\n",
    "### How Positional Encoding Works\n",
    "\n",
    "The original Transformer model by **Vaswani et al. (2017)** used sinusoidal positional encodings, where each token’s position is represented using **sine and cosine functions**:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( pos \\) is the position index of the token.\n",
    "- \\( i \\) is the dimension index.\n",
    "- \\( d_{model} \\) is the embedding dimension.\n",
    "\n",
    "This function ensures that the positional encodings are **unique** for each token and allow the model to generalize to longer sequences.\n",
    "\n",
    "### Alternative Positional Encoding Techniques\n",
    "\n",
    "Several modifications to the original positional encoding have been introduced:\n",
    "\n",
    "- **Absolute Positional Embeddings**: Used in BERT and RoBERTa, where each position has a learned embedding.\n",
    "- **Rotary Position Embeddings (RoPE)**: Introduced in later models (e.g., GPT-4), encoding relative positions to capture long-range dependencies more effectively.\n",
    "- **No Positional Encoding (NoPE)**: Some researchers argue that self-attention implicitly learns position information, reducing the need for explicit encoding.\n",
    "\n",
    "### Visualization of Positional Encoding\n",
    "\n",
    "The effect of positional encoding can be visualized using **heatmaps**. Models like **exBERT** and **BertViz** allow us to inspect attention maps, revealing how tokens attend to others based on their position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce0809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
