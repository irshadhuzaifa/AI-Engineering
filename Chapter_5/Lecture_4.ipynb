{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a81ddd6",
   "metadata": {},
   "source": [
    "# Lecture 4: Understanding Transformer Variants – BERT, GPT, T5, and More\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Transformer architecture has given rise to numerous variants, each optimized for specific tasks. These variants fall into three primary categories:\n",
    "\n",
    "- **Encoder-only models** (e.g., BERT, RoBERTa) – Specialized in understanding input text.\n",
    "- **Decoder-only models** (e.g., GPT, Transformer-XL) – Optimized for text generation.\n",
    "- **Encoder-decoder models** (e.g., T5, BART) – Designed for sequence-to-sequence tasks like translation and summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoder-Only Transformers: BERT and Its Variants\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is an encoder-based model designed to deeply understand language context by processing input text **bidirectionally**. Unlike earlier models, which read text either left-to-right (**GPT**) or right-to-left (**traditional LMs**), BERT reads both directions simultaneously, allowing it to understand word meanings in context.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Masked Language Modeling (MLM):** BERT randomly masks words in a sentence and learns to predict them based on surrounding words.\n",
    "- **Next Sentence Prediction (NSP):** Helps BERT understand sentence relationships.\n",
    "\n",
    "### Variants of BERT\n",
    "\n",
    "Several variations of BERT improve efficiency and performance:\n",
    "\n",
    "- **RoBERTa (Robustly Optimized BERT Approach):** Trained with more data and no NSP, achieving better results.\n",
    "- **ALBERT (A Lite BERT):** Reduces model size via parameter sharing.\n",
    "- **ELECTRA:** Uses a more efficient pretraining approach by replacing masked tokens instead of predicting them.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Sentiment analysis\n",
    "- Named entity recognition (NER)\n",
    "- Question answering (e.g., Google Search)\n",
    "- Text classification\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decoder-Only Transformers: GPT and Its Successors\n",
    "\n",
    "### What is GPT?\n",
    "\n",
    "**GPT (Generative Pre-trained Transformer)** is a decoder-only model optimized for **text generation**. Unlike BERT, which is **bidirectional**, GPT only processes text **left-to-right**, making it ideal for tasks like writing, storytelling, and chatbot applications.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Autoregressive Language Modeling (AR):** Predicts the next word in a sequence.\n",
    "- **Unidirectional Processing:** Uses previous words to generate the next word.\n",
    "\n",
    "### GPT Variants\n",
    "\n",
    "- **GPT-2 (2019):** Introduced larger models and few-shot learning.\n",
    "- **GPT-3 (2020):** 175B parameters, improved zero-shot learning.\n",
    "- **GPT-4 (2023):** Multimodal capabilities (text + images).\n",
    "- **GPT-4o (2024):** More efficient, faster inference.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Text completion (e.g., ChatGPT)\n",
    "- Conversational AI\n",
    "- Creative writing and storytelling\n",
    "- Code generation\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Encoder-Decoder Transformers: T5, BART, and More\n",
    "\n",
    "### What is T5?\n",
    "\n",
    "**T5 (Text-to-Text Transfer Transformer)** is a sequence-to-sequence model that treats all NLP tasks as **text-to-text problems**. Whether performing **translation, summarization, or classification**, T5 reformulates every task into a single unified format.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Denoising Pretraining:** Learns by reconstructing corrupted input sequences.\n",
    "- **Task Prefixing:** Uses explicit instructions like *\"Translate English to German:\"* to guide model behavior.\n",
    "\n",
    "### Other Encoder-Decoder Models\n",
    "\n",
    "- **BART (Bidirectional and Auto-Regressive Transformer):** Uses both **bidirectional** (like BERT) and **autoregressive** (like GPT) objectives.\n",
    "- **PEGASUS:** Optimized for **abstractive summarization**.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Machine translation (e.g., Google Translate)\n",
    "- Text summarization (e.g., news summarization)\n",
    "- Data-to-text generation\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advanced Transformer Variants\n",
    "\n",
    "### XLNet\n",
    "\n",
    "**XLNet** combines the benefits of **BERT and GPT** by using **permutation-based training**, allowing it to capture bidirectional context without relying on masking techniques.\n",
    "\n",
    "### Transformer-XL\n",
    "\n",
    "**Transformer-XL** improves **long-context modeling** by introducing a **segment recurrence mechanism**, allowing it to capture dependencies beyond fixed-length segments.\n",
    "\n",
    "### Mixture of Experts (MoE) Models\n",
    "\n",
    "Newer architectures, such as **GPT-4o, Switch Transformers, and Mixtral**, use **conditional computation** to activate only relevant parameters, improving efficiency for large-scale AI models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5d20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
