{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f340d2f1",
   "metadata": {},
   "source": [
    "# Lecture 2: How Self-Attention Works in Transformers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "At the core of Transformer models is the **self-attention** mechanism, enabling the model to evaluate the importance of each word in a sequence relative to others, regardless of their position. Unlike RNNs and LSTMs, Transformers process entire sequences in parallel, leading to greater computational efficiency and better performance in understanding contextual relationships.\n",
    "\n",
    "Self-attention empowers models such as BERT, GPT, and T5 to excel at tasks like translation, summarization, and question answering.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Self-Attention: Key Concepts\n",
    "\n",
    "Self-attention works by projecting each input token into three distinct vectors:\n",
    "\n",
    "- **Query (Q)**: Represents the current word being evaluated.\n",
    "- **Key (K)**: Represents all words in the sequence.\n",
    "- **Value (V)**: Contains the actual information for each word.\n",
    "\n",
    "These vectors are generated by multiplying the input embeddings with learned weight matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Process of Self-Attention\n",
    "\n",
    "### 1. Computing Attention Scores\n",
    "\n",
    "Calculate the dot product of each Query vector with every Key vector to measure similarity:\n",
    "\n",
    "$$\n",
    "\\text{score} = QK^T\n",
    "$$\n",
    "\n",
    "This produces a score matrix indicating the attention relevance between all token pairs.\n",
    "\n",
    "### 2. Scaling the Scores\n",
    "\n",
    "Scale the dot product scores to avoid extremely large values that can negatively impact training stability:\n",
    "\n",
    "$$\n",
    "\\text{scaled_score} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Where \\( d_k \\) is the dimensionality of the Key vectors.\n",
    "\n",
    "### 3. Applying Softmax\n",
    "\n",
    "Normalize the scaled scores into a probability distribution:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "This produces attention weights that emphasize more relevant words.\n",
    "\n",
    "### 4. Computing the Final Attention Output\n",
    "\n",
    "Multiply the attention weights with the Value vectors to aggregate contextual information:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Each output vector is a weighted sum of the value vectors, emphasizing relevant words.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "To allow the model to capture diverse types of relationships, Transformers use multiple self-attention operations in parallel:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "Each head uses different projections of Q, K, and V and captures unique aspects of word dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualization of Self-Attention\n",
    "\n",
    "Tools like **exBERT** and **BertViz** can visualize self-attention heatmaps, showing how each word in a sentence relates to others. For instance, in the sentence:\n",
    "\n",
    "> \"Mark told Sam that he was leaving.\"\n",
    "\n",
    "A well-trained model may show strong attention from “he” to “Mark”.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Self-Attention Revolutionized AI\n",
    "\n",
    "- **Parallelism**: Processes entire sequences simultaneously.\n",
    "- **Long-Range Dependencies**: Captures relationships across distant tokens.\n",
    "- **Scalability**: Suitable for large-scale training.\n",
    "- **Versatility**: Powers tasks in NLP, vision, and beyond.\n",
    "\n",
    "---\n",
    "\n",
    "Self-attention lies at the heart of modern AI, making Transformers the foundation of state-of-the-art language and multimodal models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
